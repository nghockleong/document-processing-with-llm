{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model = \"deepseek-r1:8b\")\n",
    "embeddings = OllamaEmbeddings(model = \"deepseek-r1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_page as key, page string as value\n",
    "def generate_page_doc_pair(loaded_document):\n",
    "    doc_pg_metadata = [pg.metadata for pg in loaded_document]\n",
    "    doc_pg_content = [pg.page_content for pg in loaded_document]\n",
    "    doc_keys = [f\"{metadata['source']}_{metadata['page']}\" for metadata in doc_pg_metadata]\n",
    "    return {pg_id: pg_doc for pg_id, pg_doc in zip(doc_keys, doc_pg_content)}\n",
    "\n",
    "# doc_page_num as key, token string as value\n",
    "def generate_pageidnum_tokenstr_pair(pageid_doc_d, chunksize = 100, chunk_overlap = 10):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunksize,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    pageidnum_tokenstr_d = {f'{pageid}_{idx}': tokenstr\\\n",
    "                            for pageid, pagestring in pageid_doc_d.items()\\\n",
    "                            for idx, tokenstr in enumerate(text_splitter.split_text(pagestring))}\n",
    "    return pageidnum_tokenstr_d\n",
    "\n",
    "'''\n",
    "uid contains info in the following format <filename>_<pg_num>_<token_num>\n",
    "present in the metadata as well for ease of retrieval\n",
    "'''\n",
    "def generate_pageidnum_doc_lst(pageid_doctokenstr_d):\n",
    "    return [Document(\n",
    "            page_content = txt,\n",
    "            metadata = dict(zip(['filepath', 'page_num', 'token_num'], uid.split('_'))), \n",
    "            id = uid\n",
    "            ) \n",
    "            for uid, txt in pageid_doctokenstr_d.items()]\n",
    "\n",
    "def obtain_tokenid_from_doc(token_lst):\n",
    "    return [token.id for token in token_lst]\n",
    "\n",
    "def add_doc_to_vector_db(vector_store, token_id, token_lst):\n",
    "    vector_store.add_documents(documents = token_lst, ids = token_id)\n",
    "\n",
    "def generate_llm_response(question, model, template, retriever):\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} |\n",
    "        ChatPromptTemplate.from_template(template) |\n",
    "        model |\n",
    "        StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name = \"esg_report_collection\",\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = \"./chroma\")\n",
    "\n",
    "# if persistent directory is not empty\n",
    "if not vector_store.get()['ids']:\n",
    "    vector_store.delete_collection()\n",
    "\n",
    "pdf_loc = 'data/'\n",
    "for file in os.listdir(pdf_loc):\n",
    "    loader = PyPDFLoader(file_path = f\"./{pdf_loc}/{file}\")\n",
    "    docs = loader.load()\n",
    "    pageid_docstring_dict = generate_page_doc_pair(docs)\n",
    "    pageid_doctokenstr_dict = generate_pageidnum_tokenstr_pair(pageid_docstring_dict, 1000, 100)\n",
    "    token_lst = generate_pageidnum_doc_lst(pageid_doctokenstr_dict)\n",
    "    token_id = obtain_tokenid_from_doc(token_lst)\n",
    "    add_doc_to_vector_db(vector_store, token_id, token_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"mmr\",\n",
    "                                      search_kwargs = {\"fetch_k\": 10, \"k\": 5})\n",
    "\n",
    "template = '''\n",
    "You are an AI assistant that specialises in extracting ESG information from ESG reports. \n",
    "You have to use information strictly from the vector store that I provided you with\n",
    "in the form of a retriever.\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context provided by company's ESG report:\n",
    "{context}\n",
    "\n",
    "Please respond in the following JSON format:\n",
    "{{\n",
    "    \"Name of ESG metric\": <name of the ESG metrics that you found here>\n",
    "    \"Description of ESG metric or exact ESG metric value\": <your metric here>\n",
    "    \"metadata\": <metadata of the document retrieved from the vector store (filename, page num etc.)>\n",
    "}}\n",
    "'''\n",
    "\n",
    "response = generate_llm_response('what is the reduction in water consumption for tesla in 2023?',\n",
    "                      model,\n",
    "                      template,\n",
    "                      retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
